{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다중분류 (뉴스 기사 분류 문제)\n",
    "이전 섹션에서 본 예제는 벡터 입력을 받아서 두개의 클래스로 분류하는 문제였다.  \n",
    "두개 이상의 클래스(종류,분류)로 나위어 질 경우 다중분류라는 방식을 사용한다.  \n",
    "로이터 뉴스를 46개의 토픽으로 분류하는 예제를 통해 다중분류에 대해 확인해보자.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로이터 데이터 셋\n",
    " - 1986년에 로이터에서 공개한 짧은 뉴스 기사와 토픽의 집합인 로이터 데이터셋을 사용.\n",
    " - 텍스트 분류를 위해 널리 사용되는 간단한 데이터셋이다.\n",
    " - 46개의 토픽이 있으며 어떤 토픽은 다른 것에 비해 데이터가 많다. 각 토픽은 훈련 세트에 최소한 10개의 샘플을 가지고 있다.  \n",
    " \n",
    "## 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 1s 1us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 245,\n",
       " 273,\n",
       " 207,\n",
       " 156,\n",
       " 53,\n",
       " 74,\n",
       " 160,\n",
       " 26,\n",
       " 14,\n",
       " 46,\n",
       " 296,\n",
       " 26,\n",
       " 39,\n",
       " 74,\n",
       " 2,\n",
       " 2,\n",
       " 14,\n",
       " 46,\n",
       " 2,\n",
       " 2,\n",
       " 86,\n",
       " 61,\n",
       " 2,\n",
       " 2,\n",
       " 14,\n",
       " 61,\n",
       " 451,\n",
       " 2,\n",
       " 17,\n",
       " 12]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=1000)\n",
    "len(train_data)\n",
    "len(test_data)\n",
    "train_data[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reuters 객체의 함수로 index list를 가져와보자\n",
    "reuters객체의 get_word_index()함수를 통해서 dictionary타입의 word_index를 가져올 수 있다.  \n",
    "word_index는 (value, key) 형태로 되어있다. 이를 key, value 형태로 바꿔보자.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index, reverse_word_index type: <class 'dict'>, <class 'dict'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# word_index : (value, key), reverse_word_index : (key, value)\n",
    "word_index = reuters.get_word_index()\n",
    "# print('word_index: \\n{0}'.format(word_index))\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# print('reverse_word_index: \\n{0}'.format(reverse_word_index))\n",
    "print('word_index, reverse_word_index type: {0}, {1}\\n'.format(type(word_index), type(reverse_word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "2\n",
      "8\n",
      "43\n",
      "10\n",
      "447\n",
      "5\n",
      "25\n",
      "207\n",
      "270\n",
      "5\n",
      "2\n",
      "111\n",
      "16\n",
      "369\n",
      "186\n",
      "90\n",
      "67\n",
      "7\n",
      "89\n",
      "5\n",
      "19\n",
      "102\n",
      "6\n",
      "19\n",
      "124\n",
      "15\n",
      "90\n",
      "67\n",
      "84\n",
      "22\n",
      "482\n",
      "26\n",
      "7\n",
      "48\n",
      "4\n",
      "49\n",
      "8\n",
      "864\n",
      "39\n",
      "209\n",
      "154\n",
      "6\n",
      "151\n",
      "6\n",
      "83\n",
      "11\n",
      "15\n",
      "22\n",
      "155\n",
      "11\n",
      "15\n",
      "7\n",
      "48\n",
      "9\n",
      "2\n",
      "2\n",
      "504\n",
      "6\n",
      "258\n",
      "6\n",
      "272\n",
      "11\n",
      "15\n",
      "22\n",
      "134\n",
      "44\n",
      "11\n",
      "15\n",
      "16\n",
      "8\n",
      "197\n",
      "2\n",
      "90\n",
      "67\n",
      "52\n",
      "29\n",
      "209\n",
      "30\n",
      "32\n",
      "132\n",
      "6\n",
      "109\n",
      "15\n",
      "17\n",
      "12\n",
      "Remove padding(0,1,2), separator: ?:\n",
      "? ? ? said as a result of its december acquisition of ? co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and ? ? revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash ? per share this year should be 2 50 to three dlrs reuter 3\n",
      "\n",
      "Remove padding(0,1,2), separator: ##:\n",
      "## ## ## said as a result of its december acquisition of ## co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and ## ## revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash ## per share this year should be 2 50 to three dlrs reuter 3\n",
      "\n",
      "Not Remove padding, separator: ##:\n",
      "the of of mln loss for plc said at only ended said of could 1 traders now april 0 a after said from 1985 and from foreign 000 april 0 prices its account year a but in this mln home an states earlier and rise and revs vs 000 its 16 vs 000 a but 3 of of several and shareholders and dividend vs 000 its all 4 vs 000 1 mln agreed of april 0 are 2 states will billion total and against 000 pct dlrs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "train_data[0]의 0, 1, 2는 '패딩', '문서 시작', '사전에 없음'을 위한 인덱스이므로 3을 뺀다.\n",
    "dictionary의 get()메소드의 2번째 인자로 '?'와 같은 값을 넣어주면, 키에 대한 값이 없을 경우 '?'를 출력한다.\n",
    "결국 패딩, 문서시작, 사전에 없음이 나올때마다 '?'와 같은 구분자가 출력되게 된다.\n",
    "'''\n",
    "for i in train_data[0]: print(i)\n",
    "print('Remove padding(0,1,2), separator: ?:')\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n",
    "print('{0}\\n'.format(decoded_newswire))\n",
    "\n",
    "print('Remove padding(0,1,2), separator: ##:')\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '##') for i in train_data[0]])\n",
    "print('{0}\\n'.format(decoded_newswire))\n",
    "\n",
    "print('Not Remove padding, separator: ##:')\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i, '##') for i in train_data[0]])\n",
    "print('{0}\\n'.format(decoded_newswire))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 준비\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=1000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "# 훈련 데이터 벡터 변환\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# 테스트 데이터 벡터 변환\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "레이블을 벡터로 바꾸는 방법은 두 가지이다. \n",
    " - 그 중 하나는 레이블의 리스트를 정수 텐서로 변환하는 것과 원-핫 인코딩을 사용하는 것이다. \n",
    " - 원-핫 인코딩이 범주형 데이터에 널리 사용되기 때문에 범주형 인코딩이라고도 부른다.\n",
    " - 원-핫 인코딩에 대한 자세한 설명은 6.1절을 참고하자. \n",
    " - 이 경우 레이블의 원-핫 인코딩은 각 레이블의 인덱스 자리는 1이고 나머지는 모두 0인 벡터이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "# 훈련 레이블 벡터 변환\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "# 테스트 레이블 벡터 변환\n",
    "one_hot_test_labels = to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미 케라스에는 이를 위한 내장 함수가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 구성\n",
    "### 클래스의 갯수 증가.. 출력공간의 차원의 증가( 2 -> 46 )\n",
    "이 토픽 분류 문제는 이전의 영화 리뷰 분류처럼 두 경우 모두 짧은 텍스트를 분류하는 것이다.  \n",
    "여기에서는 새로운 제약 사항이 추가되었다. 바로 출력 클래스의 개수가 2에서 46개로 늘어난 점이다.  \n",
    "그래서 출력 공간의 차원이 훨씬 커졌다.\n",
    "\n",
    "### 출력공간 차원의 증가로 중간층의 차원의 변경이 필요하다..(정보의 누락, 병목현상을 피하기 위해)\n",
    "이전에 사용했던 것처럼 Dense 층을 쌓으면 각 층은 이전 층의 출력에서 제공한 정보만 사용할 수 있다.  \n",
    "한 층이 분류 문제에 필요한 일부 정보를 누락하면 그 다음 층에서 이를 복원할 방법이 없다.  \n",
    "각 층은 잠재적으로 정보의 병목이 될 수 있다. 이전 예제에서 16차원을 가진 중간층을 사용했지만 16차원 공간은 46개의 클래스를 구분하기에 너무 제약이 많을 것 같다. 이렇게 규모가 작은 층은 유용한 정보를 완전히 잃게 되는 정보의 병목 지점처럼 동작할 수 있다.\n",
    "\n",
    "이런 이유로 좀 더 규모가 큰 층을 사용하자. 64개의 유닛을 사용해 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(1000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 구조에서 주목해야 할 점이 두 가지 있습니다:\n",
    "  \n",
    "- 마지막 Dense 층의 크기가 46입니다. 각 입력 샘플에 대해서 46차원의 벡터를 출력한다는 뜻입니다. 이 벡터의 각 원소(각 차원)은 각기 다른 출력 클래스가 인코딩된 것입니다.\n",
    "- 마지막 층에 softmax 활성화 함수가 사용되었습니다. MNIST 예제에서 이런 방식을 보았습니다. 각 입력 샘플마다 46개의 출력 클래스에 대한 확률 분포를 출력합니다. 즉, 46차원의 출력 벡터를 만들며 output[i]는 어떤 샘플이 클래스 i에 속할 확률입니다. 46개의 값을 모두 더하면 1이 됩니다.\n",
    "  \n",
    "이런 문제에 사용할 최선의 손실 함수는 categorical_crossentropy입니다. 이 함수는 두 확률 분포의 사이의 거리를 측정합니다. 여기에서는 네트워크가 출력한 확률 분포와 진짜 레이블의 분포 사이의 거리입니다. 두 분포 사이의 거리를 최소화하면 진짜 레이블에 가능한 가까운 출력을 내도록 모델을 훈련하게 됩니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
