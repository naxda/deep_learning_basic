{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 과대적합과 과소적합\n",
    "이전의 영화리뷰 분류, 뉴스기사 분류, 세가지 예제에서 모델의 성능이 몇번째 에포크 이후에 최고치에 다다랐다가 감소하기 시작했다.  \n",
    "즉, 모델이 과대 적합되기 시작한 것이다. 머신러닝에서는 과대적합을 다룰 줄 알아야 한다.\n",
    "\n",
    "## 최적화와 일반화\n",
    " - 최적화는 가능한 훈련 데이터에서 최고의 성능을 얻기위해 모델을 조정하는 과정이다.(머신러닝의 학습).  \n",
    " - 반면 일반화는 훈련된 모델이 이전에 본 적없는 데이터에서 얼마나 잘 수행되는지를 의미한다.\n",
    " - 모델을 만드는 목적은 좋은 일반화 성능을 얻는 것인데, 일반화 성능을 직접적으로 제어할 방법이 없고 훈련데이터를 기반으로 모델을 조정할 수 있다.  \n",
    "\n",
    "## 과소적합, 과대적합\n",
    " - 훈련 초기에는 최적화와 일반화가 상호연관되어 있다. **훈련데이터의 손실이 낮을 수록 데이터의 손실이 낮은데, 이런 증상을 과소적합되었다라고 말한다.** 네트워크가 훈련 데이터에 있는 관련 특성을 모두 학습하지 못했기 때문에 모델의 성능이 계속 발전될 여지가 있다. \n",
    " - 하지만 훈련데이터에 여러번 반복학습하고 나면 어느시점부터 일반화 성능이 더이상 높아지지 않는다. **검증세트의 성능이 멈추고 감소되기 시작하는데 이런 증상을 과대적합되었다 라고 말한다**. 이는 훈련데이터에 특화된 패턴을 모델이 학습하기 시작했다는 의미이며, 새로운 데이터에 대해 잘못된 판단을 할 확률이 높아진다.\n",
    "\n",
    "## 과소적합, 과대적합을 피하려면..\n",
    "- 모델이 관련성이 없고 좋지 못한 패턴을 학습하지 못하도록 하기 위한 방법은 더 많은 훈련 데이터를 모으는 것이다. 당연한 얘기지만, 더 많은 데이터에서 훈련된 모델은 일반화 성능이 더 뛰어나다.\n",
    "- **데이터를 더 모으는 것이 불가능할 때, 차선책은 모델이 수용할 수 있는 정보의 양을 조절하거나 저장할 수 있는 정보에 제약을 가하는 것**이다. 네트워크가 **적은 수의 패턴**만을 기억할 수 있다면, 가장 중요한 패턴에 집중하게 될 것이고 이런 패턴은 더 나은 일반화 성능을 제공할 수 있다.\n",
    "\n",
    "## 데이터 준비\n",
    "IMDB 데이터 세트를 이용해서 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # 크기가 (len(sequences), dimension)이고 모든 원소가 0인 행렬을 만든다\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1. # results[i]에서 특정 인덱스의 위치를 1로 만든다.\n",
    "    return results\n",
    "\n",
    "# 훈련데이터를 벡터로 변환한다.\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# 테스트 데이터를 벡터로 변환한다.\n",
    "x_test = vectorize_sequences(test_data)\n",
    "#레이블을 벡터로 변한한다.\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 과대적합을 피하기 위한 방법\n",
    "### 네트워크 크기 축소\n",
    " - **과대적합을 막는 가장 단순한 방법은 모델의 크기, 즉 모델에 있는 학습 파라미터의 수를 줄이는 것**이다.\n",
    " - 파라미터의 수는 **층의 수와 각 층의 유닛 수**에 의해 결정된다. 딥러닝에서 모델에 있는 학습 파라미터의 수를 용량이라고 한다. 그리고 당연하게 파라미터가 많은 모델이 기억용량이 더 많다.\n",
    " - 딥러닝 모델은 훈련데이터에 잘 맞으려는 경향을 가진다. 이 문제는 최적화가 아니라 일반화이다.\n",
    " - 결국, 과대적합을 피하고 일반화를 하기위해서는 알맞은 층의 수나 각 층의 유닛수를 결정하면 된다. 그런데, 알맞은 층의 수나 각 층의 유닛 수를 결정할 수 있는 마법같은 공식은 없다. **데이터에 알맞는 모델 크기를 찾으려면 각기 다른 구조를 평가해보면서 알맞은 층의 수나 유닛 수를 조절해나가야한다.**\n",
    " - **일반적으로 비교적 작은 층과 파라미터로 시작해서 검증 손실이 감소되기 시작할때까지 유닛의 수를 늘리는 것이다.**\n",
    " \n",
    "이렇게 네트워크 크기를 축소해서 과대적합 문제를 해결하는 과정을 예제로 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "original_model = models.Sequential()\n",
    "original_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "original_model.add(layers.Dense(16, activation='relu'))\n",
    "original_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "original_model.compile(optimizer='rmsprop',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더 작은 네트워크로 바꿔보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_model = models.Sequential()\n",
    "smaller_model.add(layers.Dense(6, activation='relu', input_shape=(10000,)))\n",
    "smaller_model.add(layers.Dense(6, activation='relu'))\n",
    "smaller_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "smaller_model.compile(optimizer='rmsprop',\n",
    "                     loss='binary_crossentropy',\n",
    "                     metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원본 네트워크와 축소된 네트워크의 검증손실을 비교해보자.  \n",
    "점으로 표현된 것이 작은 네트워크이고 덧셈기호가 원래 네트워크이다.  \n",
    "(검증 손실이 작은 것이 좋은 모델이다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 4s 147us/step - loss: 0.4440 - acc: 0.8251 - val_loss: 0.3286 - val_acc: 0.8835\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 2s 95us/step - loss: 0.2573 - acc: 0.9078 - val_loss: 0.2864 - val_acc: 0.8882\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 3s 120us/step - loss: 0.1991 - acc: 0.9292 - val_loss: 0.2822 - val_acc: 0.8892\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 2s 95us/step - loss: 0.1666 - acc: 0.9414 - val_loss: 0.2940 - val_acc: 0.8846\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 3s 119us/step - loss: 0.1434 - acc: 0.9502 - val_loss: 0.3115 - val_acc: 0.8807\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 2s 95us/step - loss: 0.1257 - acc: 0.9560 - val_loss: 0.3485 - val_acc: 0.8719\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 3s 121us/step - loss: 0.1109 - acc: 0.9616 - val_loss: 0.3581 - val_acc: 0.8732\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 3s 111us/step - loss: 0.0978 - acc: 0.9670 - val_loss: 0.3966 - val_acc: 0.8659\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 3s 132us/step - loss: 0.0840 - acc: 0.9724 - val_loss: 0.4133 - val_acc: 0.8674\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 3s 109us/step - loss: 0.0754 - acc: 0.9758 - val_loss: 0.4830 - val_acc: 0.8565\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 3s 129us/step - loss: 0.0686 - acc: 0.9782 - val_loss: 0.4993 - val_acc: 0.8569\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 3s 102us/step - loss: 0.0565 - acc: 0.9832 - val_loss: 0.5201 - val_acc: 0.8566\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 3s 126us/step - loss: 0.0518 - acc: 0.9840 - val_loss: 0.5335 - val_acc: 0.8584\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 2s 97us/step - loss: 0.0428 - acc: 0.9877 - val_loss: 0.6035 - val_acc: 0.8514\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 3s 121us/step - loss: 0.0360 - acc: 0.9899 - val_loss: 0.6118 - val_acc: 0.8547\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 2s 98us/step - loss: 0.0313 - acc: 0.9908 - val_loss: 0.6434 - val_acc: 0.8529\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 3s 120us/step - loss: 0.0270 - acc: 0.9924 - val_loss: 0.7626 - val_acc: 0.8430\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 2s 96us/step - loss: 0.0206 - acc: 0.9950 - val_loss: 0.7204 - val_acc: 0.8509\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 3s 120us/step - loss: 0.0204 - acc: 0.9948 - val_loss: 0.7598 - val_acc: 0.8480\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 2s 97us/step - loss: 0.0144 - acc: 0.9969 - val_loss: 0.8235 - val_acc: 0.8463\n"
     ]
    }
   ],
   "source": [
    "original_hist = original_model.fit(x_train, y_train,\n",
    "                                  epochs=20,\n",
    "                                  batch_size=512,\n",
    "                                  validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 3s 126us/step - loss: 0.5642 - acc: 0.7422 - val_loss: 0.4789 - val_acc: 0.8314\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 2s 92us/step - loss: 0.3865 - acc: 0.8908 - val_loss: 0.3570 - val_acc: 0.8795\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 3s 117us/step - loss: 0.2763 - acc: 0.9171 - val_loss: 0.3005 - val_acc: 0.8877\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 2s 92us/step - loss: 0.2208 - acc: 0.9282 - val_loss: 0.2828 - val_acc: 0.8895\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 3s 119us/step - loss: 0.1883 - acc: 0.9373 - val_loss: 0.2815 - val_acc: 0.8868\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 3s 100us/step - loss: 0.1658 - acc: 0.9450 - val_loss: 0.2850 - val_acc: 0.8848\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 3s 123us/step - loss: 0.1478 - acc: 0.9514 - val_loss: 0.2940 - val_acc: 0.8822\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 2s 93us/step - loss: 0.1344 - acc: 0.9554 - val_loss: 0.3052 - val_acc: 0.8804\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 3s 123us/step - loss: 0.1212 - acc: 0.9615 - val_loss: 0.3198 - val_acc: 0.8779\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 2s 98us/step - loss: 0.1112 - acc: 0.9645 - val_loss: 0.3326 - val_acc: 0.8754\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 3s 124us/step - loss: 0.1006 - acc: 0.9684 - val_loss: 0.3504 - val_acc: 0.8721\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 2s 96us/step - loss: 0.0913 - acc: 0.9723 - val_loss: 0.3718 - val_acc: 0.8700\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 3s 122us/step - loss: 0.0828 - acc: 0.9756 - val_loss: 0.3847 - val_acc: 0.8689\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 3s 112us/step - loss: 0.0752 - acc: 0.9768 - val_loss: 0.4047 - val_acc: 0.8664\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 3s 127us/step - loss: 0.0678 - acc: 0.9804 - val_loss: 0.4301 - val_acc: 0.8631\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 3s 108us/step - loss: 0.0605 - acc: 0.9834 - val_loss: 0.4529 - val_acc: 0.8610\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 3s 123us/step - loss: 0.0544 - acc: 0.9851 - val_loss: 0.4645 - val_acc: 0.8623\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 2s 94us/step - loss: 0.0485 - acc: 0.9878 - val_loss: 0.4962 - val_acc: 0.8584\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 3s 118us/step - loss: 0.0434 - acc: 0.9893 - val_loss: 0.5113 - val_acc: 0.8591\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 3s 135us/step - loss: 0.0379 - acc: 0.9909 - val_loss: 0.5543 - val_acc: 0.8549\n"
     ]
    }
   ],
   "source": [
    "smaller_model_hist = smaller_model.fit(x_train, y_train,\n",
    "                                      epochs=20,\n",
    "                                      batch_size=512,\n",
    "                                      validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.pyplot' has no attribute 'legent'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7b0009c8ca0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Validation loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.pyplot' has no attribute 'legent'"
     ]
    }
   ],
   "source": [
    "epochs=range(1,21)\n",
    "original_val_loss = original_hist.history['val_loss']\n",
    "smaller_model_val_loss = smaller_model_hist.history['val_loss']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# 'b+'는 파란색 덧셈 기호를 의미한다.\n",
    "plt.plot(epochs, original_val_loss, 'b+', label='Original model')\n",
    "# 'bo'는 파란색 점을 의미합니다.\n",
    "plt.plot(epochs, smaller_model_val_loss, 'bo', label='Smaller model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
