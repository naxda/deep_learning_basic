{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5_1_logistic_classification_diabetes_eager_execution.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vUEYc6wKa-Ra","colab_type":"code","outputId":"acf9b4de-918b-42c8-dbe0-35d7f9febecf","executionInfo":{"status":"ok","timestamp":1557928581608,"user_tz":-540,"elapsed":14384,"user":{"displayName":"SoonYoung Jung","photoUrl":"","userId":"08526165032976493552"}},"colab":{"base_uri":"https://localhost:8080/","height":612}},"source":["#logistic classification을 diabetes data를 활용해서 모델을 만들어보자\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import tensorflow.contrib.eager as tfe\n","\n","tf.enable_eager_execution()\n","tf.set_random_seed(777)\n","print(tf.__version__)\n","\n","\n","# data\n","xy = np.loadtxt('data-03-diabetes.csv', delimiter=',', dtype=np.float32)\n","x_train = xy[:, 0:-1]\n","y_train = xy[:, [-1]]\n","\n","print('xy:\\n{}'.format(xy))\n","print('x_train:\\n{}'.format(x_train))\n","# print('y_train:\\n{}'.format(y_train))\n","print(x_train.shape, y_train.shape)\n","\n","# initialize dataset, W, b\n","dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))\n","W = tf.Variable(tf.random_normal([8, 1]), name='weight')\n","b = tf.Variable(tf.random_normal([1]), name='bias')\n","\n","# hypothesis로는 WX+b를 Sigmoid activation을 취한 값을 사용\n","# Sigmoid: 1 / (1 + exp(Wx+b))\n","# z = Wx+b, g(z), g(z) = 1/(1+exp(z))\n","def logistic_regression(features):\n","  hypothesis = tf.div(1., 1. + tf.exp(tf.matmul(features, W) + b))\n","  return hypothesis\n","\n","# 손실 함수로는 crossentropy 함수를 사용한다.\n","# crossentropy함수는 y * log(hypothesis + (1-y)*log(1-hypothesis))\n","def loss_fn(features, labels):\n","  hypothesis = logistic_regression(features)\n","  loss = -tf.reduce_mean(labels * tf.log(logistic_regression(features)) + (1-labels) * tf.log(1 - hypothesis))\n","  return loss\n","\n","# 손실 값의 gradient를 계산한다.\n","def grad(features, labels):\n","  hypothesis = logistic_regression(features)\n","  with tf.GradientTape() as tape:\n","    loss_value = loss_fn(features, labels)\n","  return tape.gradient(loss_value, [W, b])\n","\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n","\n","# 1000번의 epoch동안 gradient를 변화시켜 가며 손실이 최소가 되는 점을 찾는다\n","EPOCHS = 1001\n","for step in range(EPOCHS):\n","  for features, labels in dataset:\n","    grads = grad(features, labels)\n","    optimizer.apply_gradients(grads_and_vars=zip(grads, [W, b]))\n","    if step % 100 == 0:\n","      print('Iter: {}, Loss: {:.4f}'.format(step, loss_fn(features, labels)))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["1.13.1\n","xy:\n","[[-0.294118   0.487437   0.180328  ... -0.53117   -0.0333333  0.       ]\n"," [-0.882353  -0.145729   0.0819672 ... -0.766866  -0.666667   1.       ]\n"," [-0.0588235  0.839196   0.0491803 ... -0.492741  -0.633333   0.       ]\n"," ...\n"," [-0.411765   0.21608    0.180328  ... -0.857387  -0.7        1.       ]\n"," [-0.882353   0.266332  -0.0163934 ... -0.768574  -0.133333   0.       ]\n"," [-0.882353  -0.0653266  0.147541  ... -0.797609  -0.933333   1.       ]]\n","x_train:\n","[[-0.294118    0.487437    0.180328   ...  0.00149028 -0.53117\n","  -0.0333333 ]\n"," [-0.882353   -0.145729    0.0819672  ... -0.207153   -0.766866\n","  -0.666667  ]\n"," [-0.0588235   0.839196    0.0491803  ... -0.305514   -0.492741\n","  -0.633333  ]\n"," ...\n"," [-0.411765    0.21608     0.180328   ... -0.219076   -0.857387\n","  -0.7       ]\n"," [-0.882353    0.266332   -0.0163934  ... -0.102832   -0.768574\n","  -0.133333  ]\n"," [-0.882353   -0.0653266   0.147541   ... -0.0938897  -0.797609\n","  -0.933333  ]]\n","(759, 8) (759, 1)\n","Iter: 0, Loss: 0.6556\n","Iter: 100, Loss: 0.6188\n","Iter: 200, Loss: 0.5980\n","Iter: 300, Loss: 0.5854\n","Iter: 400, Loss: 0.5769\n","Iter: 500, Loss: 0.5704\n","Iter: 600, Loss: 0.5648\n","Iter: 700, Loss: 0.5599\n","Iter: 800, Loss: 0.5555\n","Iter: 900, Loss: 0.5513\n","Iter: 1000, Loss: 0.5475\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OgqGrMzjeRi9","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}