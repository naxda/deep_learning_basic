{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"05_3_logistic_classification_mnist.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vUEYc6wKa-Ra","colab_type":"code","outputId":"af842959-fcae-4c0d-8a81-a2a855e92285","executionInfo":{"status":"ok","timestamp":1558283724352,"user_tz":-540,"elapsed":115163,"user":{"displayName":"SoonYoung Jung","photoUrl":"","userId":"08526165032976493552"}},"colab":{"base_uri":"https://localhost:8080/","height":734}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.utils import to_categorical\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# model function\n","def create_model():\n","    model = keras.Sequential()\n","    model.add(keras.layers.Flatten(input_shape=(28,28)))\n","    model.add(keras.layers.Dense(10, kernel_initializer=tf.keras.initializers.RandomNormal()))\n","    '''\n","    https://keras.io/layers/core/\n","    keras.layers.Dense(units, , , kernel_initializer='glorot_uniform',, , , , ,)\n","    units : 양의 정수, 출력 공간의 차원.\n","    kernel_initializer : kernel가중치 행렬에 대한 이니셜 라이저 ( 이니셜 라이저 참조 ).\n","    [입력 모양]\n","    모양이있는 nD 텐서 : (batch_size, ..., input_dim). 가장 일반적인 상황은 모양이있는 2D 입력입니다 (batch_size, input_dim).\n","    [출력 형태]\n","    모양이있는 nD 텐서 : (batch_size, ..., units). \n","    예를 들어, 모양이있는 2D 입력의 (batch_size, input_dim)경우, \n","    출력 형태는 (batch_size, units)됩니다.\n","    결국, 여기서 출력형태는 (100, 10)이 된다.\n","    '''\n","    return model\n","\n","# loss function, 손실함수로 cross entropy를 사용한다\n","def loss_fn(model, images, labels):\n","    logits = model(images, training=True)\n","    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n","            logits=logits, labels=labels))    \n","    return loss\n","\n","# calculating gradient, 손실 값의 gradient를 구한다\n","def grad(model, images, labels):\n","    with tf.GradientTape() as tape:\n","        loss = loss_fn(model, images, labels)\n","    return tape.gradient(loss, model.variables)\n","\n","# calculating model's accuracy\n","def evaluate(model, images, labels):#images, labels: (100, 28, 28), (100, 10)\n","#     print('[shape] evaluate, images shape: {0}'.format(images.shape))#(100, 28, 28)\n","#     print('[shape] evaluate, labels shape: {0}'.format(labels.shape))#(100, 10)\n","    logits = model(images, training=False)\n","#     print('[shape] evaluate, logits shape: {0}'.format(logits.shape))#(100, 10)\n","    # batch_size: 100, units는 10이므로 logits의 형태는 (100, 10)이 된다.위의 create_model 참고\n","    '''\n","    https://www.tensorflow.org/api_docs/python/tf/math/argmax\n","    tf.argmax(input, axis=none): input안에서 axis=0:각 열에서 큰 값(행인덱스)\n","    axis=1: 행에서 가장 큰값을 찾는다.\n","    '''\n","    # logits shape: (100,10), label shape: (100, 10)\n","#     print('[evaluate] logits, labels shape: {0}, {1}'.format(logits.shape, labels.shape))\n","#     print('[evaluate] logits data: {0}'.format(logits[0]))\n","#     print('[evaluate] labels data: {0}'.format(labels[0]))\n","    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n","    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","    return accuracy\n","\n","def show_mnist_dataset(class_labels, train_images):\n","  # print dataset\n","  plt.figure()\n","  plt.imshow(train_images[0], cmap=plt.cm.binary)\n","  plt.colorbar()\n","  \n","  plt.figure(figsize=(15,15))\n","  for i in range(25):\n","      plt.subplot(5,5,i+1)\n","      plt.xticks([])\n","      plt.yticks([])\n","      plt.grid(False)\n","      plt.imshow(train_images[i], cmap=plt.cm.binary)\n","      plt.xlabel(class_labels[train_labels[i]])\n","\n","def load_mnist_dataset(class_labels):\n","  # load dataset from keras api\n","  mnist = keras.datasets.mnist\n","  class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n","  # copy class name list to argument\n","  for i in range(len(class_names)):\n","    class_labels.append(class_names[i])\n","  print('class_labels: {0}, class_names: {1}'.format(class_labels, class_names))\n","  print('type: {0}, {1}'.format(type(class_names), type(class_labels)))\n","  # load datasets\n","  (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n","  print('[type]  train_image : {0}, train_labels: {1}'.format(type(train_images), type(train_labels)))\n","  print('[shape] train_images: {0}, train_labels: {1}'.format(train_images.shape, train_labels.shape))\n","  print('[shape] test_images : {0}, test_labels : {1}'.format(test_images.shape, test_labels.shape))\n","  return (train_images, train_labels), (test_images, test_labels)\n","\n","def load_fashion_mnist_dataset(class_labels):\n","  # load dataset from keras api\n","  mnist = keras.datasets.fashion_mnist\n","  class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n","  # copy class name list to argument\n","  for i in range(len(class_names)):\n","    class_labels.append(class_names[i])\n","  print('class_labels: {0}, class_names: {1}'.format(class_labels, class_names))\n","  print('type: {0}, {1}'.format(type(class_names), type(class_labels)))\n","  # load datasets\n","  (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n","  print('[type]  train_images: {0}, train_labels: {1}'.format(type(train_images), type(train_labels)))\n","  print('[shape] train_images: {0}, train_labels: {1}'.format(train_images.shape, train_labels.shape))\n","  print('[shape] test_images : {0}, test_labels : {1}'.format(test_images.shape, test_labels.shape))\n","  return (train_images, train_labels), (test_images, test_labels)\n","\n","print(tf.__version__)\n","print(keras.__version__)\n","\n","tf.enable_eager_execution()\n","\n","# hyper parameters\n","learning_rate = 0.001\n","training_epochs = 20\n","batch_size = 100\n","tf.set_random_seed(777)\n","\n","# load mnist datasets\n","class_labels = []\n","(train_images, train_labels), (test_images, test_labels) = load_mnist_dataset(class_labels)\n","show_mnist_dataset(class_labels=class_labels, train_images=train_images)\n","\n","# convert dataset value to number in 0~1 \n","# one hot encoding\n","train_images = train_images.astype(np.float32) / 255.\n","test_images = test_images.astype(np.float32) / 255.\n","    \n","train_labels = to_categorical(train_labels, 10)\n","test_labels = to_categorical(test_labels, 10)    \n","    \n","train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(\n","                buffer_size=100000).batch(batch_size)\n","test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(batch_size)\n","\n","\n","# create model\n","model = create_model()\n","model.summary()\n","print('model type: {}'.format(type(model)))\n","\n","# optimizer\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","\n","# training\n","print('Learning started. It takes sometime.')\n","for epoch in range(training_epochs):\n","    avg_loss = 0.\n","    avg_train_acc = 0.\n","    avg_test_acc = 0.\n","    train_step = 0\n","    test_step = 0\n","    \n","    for images, labels in train_dataset:\n","        grads = grad(model, images, labels)                \n","        optimizer.apply_gradients(zip(grads, model.variables))\n","        loss = loss_fn(model, images, labels)\n","        acc = evaluate(model, images, labels)\n","        avg_loss = avg_loss + loss\n","        avg_train_acc = avg_train_acc + acc\n","        train_step += 1\n","    avg_loss = avg_loss / train_step\n","    avg_train_acc = avg_train_acc / train_step\n","    \n","    for images, labels in test_dataset:\n","#         print('[shape](test set) images, labels: {0}, {1}'.format(images.shape, labels.shape))\n","        acc = evaluate(model, images, labels)\n","        avg_test_acc = avg_test_acc + acc\n","        test_step += 1\n","    avg_test_acc = avg_test_acc / test_step    \n","\n","    print('Epoch:', '{}'.format(epoch + 1), 'loss =', '{:.8f}'.format(avg_loss), \n","          'train accuracy = ', '{:.4f}'.format(avg_train_acc), \n","          'test accuracy = ', '{:.4f}'.format(avg_test_acc))\n","\n","\n","print('Learning Finished!')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.13.1\n","2.2.4-tf\n","class_labels: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], class_names: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n","type: <class 'list'>, <class 'list'>\n","[type]  train_image : <class 'numpy.ndarray'>, train_labels: <class 'numpy.ndarray'>\n","[shape] train_images: (60000, 28, 28), train_labels: (60000,)\n","[shape] test_images : (10000, 28, 28), test_labels : (10000,)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:642: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","flatten (Flatten)            (None, 784)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 10)                7850      \n","=================================================================\n","Total params: 7,850\n","Trainable params: 7,850\n","Non-trainable params: 0\n","_________________________________________________________________\n","model type: <class 'tensorflow.python.keras.engine.sequential.Sequential'>\n","Learning started. It takes sometime.\n","Epoch: 1 loss = 0.61409241 train accuracy =  0.8487 test accuracy =  0.9043\n","Epoch: 2 loss = 0.34338385 train accuracy =  0.9064 test accuracy =  0.9135\n","Epoch: 3 loss = 0.30736655 train accuracy =  0.9149 test accuracy =  0.9184\n","Epoch: 4 loss = 0.29043213 train accuracy =  0.9193 test accuracy =  0.9214\n","Epoch: 5 loss = 0.28015631 train accuracy =  0.9216 test accuracy =  0.9231\n","Epoch: 6 loss = 0.27307636 train accuracy =  0.9235 test accuracy =  0.9240\n","Epoch: 7 loss = 0.26780605 train accuracy =  0.9251 test accuracy =  0.9251\n","Epoch: 8 loss = 0.26367453 train accuracy =  0.9263 test accuracy =  0.9257\n","Epoch: 9 loss = 0.26031485 train accuracy =  0.9276 test accuracy =  0.9263\n","Epoch: 10 loss = 0.25750741 train accuracy =  0.9287 test accuracy =  0.9261\n","Epoch: 11 loss = 0.25511193 train accuracy =  0.9295 test accuracy =  0.9260\n","Epoch: 12 loss = 0.25303432 train accuracy =  0.9303 test accuracy =  0.9259\n","Epoch: 13 loss = 0.25120795 train accuracy =  0.9310 test accuracy =  0.9261\n","Epoch: 14 loss = 0.24958448 train accuracy =  0.9317 test accuracy =  0.9258\n","Epoch: 15 loss = 0.24812788 train accuracy =  0.9324 test accuracy =  0.9265\n","Epoch: 16 loss = 0.24681062 train accuracy =  0.9327 test accuracy =  0.9264\n","Epoch: 17 loss = 0.24561106 train accuracy =  0.9331 test accuracy =  0.9265\n","Epoch: 18 loss = 0.24451202 train accuracy =  0.9333 test accuracy =  0.9265\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OgqGrMzjeRi9","colab_type":"code","colab":{}},"source":[" "],"execution_count":0,"outputs":[]}]}