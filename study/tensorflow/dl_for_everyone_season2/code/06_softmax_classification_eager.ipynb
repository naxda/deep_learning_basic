{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6_softmax_classification_eager.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Bngi9BpVCwCT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":955},"outputId":"16e0dc65-dead-4972-a21d-6e102e6921a8","executionInfo":{"status":"ok","timestamp":1557753675397,"user_tz":-540,"elapsed":8305,"user":{"displayName":"SoonYoung Jung","photoUrl":"","userId":"08526165032976493552"}}},"source":["import tensorflow as tf\n","import numpy as np\n","tf.enable_eager_execution()\n","tf.set_random_seed(777)\n","tf.enable_eager_execution()\n","\n","x_data = [[1, 2, 1, 1],\n","          [2, 1, 3, 2],\n","          [3, 1, 3, 4],\n","          [4, 1, 5, 5],\n","          [1, 7, 5, 5],\n","          [1, 2, 5, 6],\n","          [1, 6, 6, 6],\n","          [1, 7, 7, 7]]\n","y_data = [[0, 0, 1],\n","          [0, 0, 1],\n","          [0, 0, 1],\n","          [0, 1, 0],\n","          [0, 1, 0],\n","          [0, 1, 0],\n","          [1, 0, 0],\n","          [1, 0, 0]]\n","\n","#convert into numpy and float format\n","x_data = np.asarray(x_data, dtype=np.float32)#(8, 4)\n","y_data = np.asarray(y_data, dtype=np.float32)#(8, 3)\n","\n","# dataset을 선언한다.\n","# dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n","# dataset = dataset.repeat().batch(2)\n","nb_classes = 3 #class의 갯수\n","print('x_data shape: {0}'.format(x_data.shape))\n","print('y_data shape: {0}'.format(y_data.shape))\n","\n","#Weight and bias setting\n","W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')#(4,3)\n","b = tf.Variable(tf.random_normal([nb_classes]), name='bias')#(3,)\n","variables = [W, b]\n","print('===== W, b =====')\n","print('@@@@ W: {0}, \\n@@@@ b: {1}'.format(W, b))\n","print('================\\n')\n","\n","\n","# tf.nn.softmax computes softmax activations\n","# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n","def hypothesis(X):\n","  return tf.nn.softmax(tf.matmul(X, W) + b)# X(8,4) * W(4,3) : (8,3)\n","print('===== hypothesis =====')\n","print('hypothesis(x_data): {0}'.format(hypothesis(x_data)))\n","print('======================\\n')\n","\n","sample_db = [[8, 2, 1, 4]]#(1,4)\n","sample_db = np.asarray(sample_db, dtype=np.float32)\n","print('===== hypothesis =====')\n","print('hypothesis(sample_db): {0}'.format(hypothesis(sample_db)))#X(1,4) * W(4,3): (1,3)\n","print('======================\\n')\n","\n","def cost_fn(X, Y):\n","  logits = hypothesis(X)\n","  cost = -tf.reduce_sum(Y * tf.log(logits), axis=1)# -1* sum(Y * log(XW+b)), sum: 차원의 총합을 계산\n","  cost_mean = tf.reduce_mean(cost)\n","  return cost_mean\n","\n","print('cost_fn: {0}'.format(cost_fn(x_data, y_data)))\n","\n","x = tf.constant(3.0)\n","with tf.GradientTape() as g:\n","  g.watch(x)\n","  y = x*x\n","  dy_dx = g.gradient(y, x)#Will compute to 6.0\n","  print('dy_dx: {0}'.format(dy_dx))\n","\n","def grad_fn(X, Y):\n","  with tf.GradientTape() as tape:\n","    loss = cost_fn(X, Y)\n","    grads = tape.gradient(loss, variables)\n","    return grads\n","print('grad_fn: {0}'.format(grad_fn(x_data, y_data)))\n","\n","def fit(X, Y, epochs=2000, verbose=100):\n","  optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n","  \n","  for i in range(epochs):\n","    grads = grad_fn(X,Y)\n","    optimizer.apply_gradients(zip(grads, variables))\n","    if (i == 0) | ((i+1)%verbose==0):\n","      print('Loss at epoch %d: %f' %(i+1, cost_fn(X, Y).numpy()))\n","\n","fit(x_data, y_data)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["x_data shape: (8, 4)\n","y_data shape: (8, 3)\n","===== W, b =====\n","@@@@ W: <tf.Variable 'weight:0' shape=(4, 3) dtype=float32, numpy=\n","array([[ 0.7706481 ,  0.37335402, -0.05576323],\n","       [ 0.00358377, -0.5898363 ,  1.5702795 ],\n","       [ 0.2460895 , -0.09918973,  1.4418385 ],\n","       [ 0.3200988 ,  0.526784  , -0.7703731 ]], dtype=float32)>, \n","@@@@ b: <tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([-1.3080608 , -0.13253094,  0.5513761 ], dtype=float32)>\n","================\n","\n","===== hypothesis =====\n","hypothesis(x_data): [[1.3657196e-02 7.9016229e-03 9.7844124e-01]\n"," [3.9259814e-02 1.7034756e-02 9.4370544e-01]\n"," [3.8038522e-01 1.6772322e-01 4.5189154e-01]\n"," [3.2339045e-01 5.9075940e-02 6.1753362e-01]\n"," [3.6299741e-06 6.2072729e-08 9.9999630e-01]\n"," [2.6252046e-02 1.0727973e-02 9.6301997e-01]\n"," [1.5652511e-05 4.2180275e-07 9.9998391e-01]\n"," [2.9407772e-06 3.8113402e-08 9.9999702e-01]]\n","======================\n","\n","===== hypothesis =====\n","hypothesis(sample_db): [[0.9302204  0.06200534 0.00777428]]\n","======================\n","\n","cost_fn: 6.079319477081299\n","dy_dx: 6.0\n","grad_fn: [<tf.Tensor: id=107473, shape=(4, 3), dtype=float32, numpy=\n","array([[ 0.06914604, -0.6509784 ,  0.5818323 ],\n","       [-1.5221258 , -1.214863  ,  2.7369888 ],\n","       [-1.2473829 , -1.7611002 ,  3.008483  ],\n","       [-1.2014607 , -1.865923  ,  3.0673838 ]], dtype=float32)>, <tf.Tensor: id=107471, shape=(3,), dtype=float32, numpy=array([-0.15212914, -0.342192  ,  0.4943211 ], dtype=float32)>]\n","Loss at epoch 1: 2.849417\n","Loss at epoch 100: 0.684151\n","Loss at epoch 200: 0.613813\n","Loss at epoch 300: 0.558204\n","Loss at epoch 400: 0.508306\n","Loss at epoch 500: 0.461059\n","Loss at epoch 600: 0.415071\n","Loss at epoch 700: 0.369636\n","Loss at epoch 800: 0.324533\n","Loss at epoch 900: 0.280721\n","Loss at epoch 1000: 0.246752\n","Loss at epoch 1100: 0.232798\n","Loss at epoch 1200: 0.221645\n","Loss at epoch 1300: 0.211476\n","Loss at epoch 1400: 0.202164\n","Loss at epoch 1500: 0.193606\n","Loss at epoch 1600: 0.185714\n","Loss at epoch 1700: 0.178415\n","Loss at epoch 1800: 0.171645\n","Loss at epoch 1900: 0.165350\n","Loss at epoch 2000: 0.159483\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3_xZYkmqGXo6","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}