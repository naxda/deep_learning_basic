{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"11_1_cnn_keras_sequential.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"YpceWr8drAiO","colab_type":"code","outputId":"e0c51563-1a9c-4d8d-85df-32a724ac6d16","executionInfo":{"status":"ok","timestamp":1555652356562,"user_tz":-540,"elapsed":743,"user":{"displayName":"SoonYoung Jung","photoUrl":"","userId":"08526165032976493552"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"cell_type":"code","source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.utils import to_categorical\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","\n","print(tf.__version__)\n","print(keras.__version__)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.13.1\n","2.2.4-tf\n"],"name":"stdout"}]},{"metadata":{"id":"hzQxnlnpesV6","colab_type":"code","colab":{}},"cell_type":"code","source":["# model 생성\n","def create_model():\n","  # layer를 stack과 비슷하게 순차적으로 쌓는다.\n","  model = keras.Sequential()\n","  # 첫 layer의 input shape는 반드시 써줘야 한다. 그 다음 layer부터는 입력해주지 않아도 된다.\n","  model.add(keras.layers.Conv2D(filters=32, kernel_size=3, activation=tf.nn.relu, padding='SAME', \n","                                input_shape=(28, 28, 1)))\n","  model.add(keras.layers.MaxPool2D(padding='SAME'))\n","  model.add(keras.layers.Conv2D(filters=64, kernel_size=3, activation=tf.nn.relu, padding='SAME'))\n","  model.add(keras.layers.MaxPool2D(padding='SAME'))\n","  model.add(keras.layers.Conv2D(filters=128, kernel_size=3, activation=tf.nn.relu, padding='SAME'))\n","  model.add(keras.layers.MaxPool2D(padding='SAME'))\n","  # fully connected layer\n","  model.add(keras.layers.Flatten())\n","  model.add(keras.layers.Dense(256, activation=tf.nn.relu))\n","  model.add(keras.layers.Dropout(0.4))\n","  model.add(keras.layers.Dense(10))\n","  return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7wQq8zkMnHy-","colab_type":"code","colab":{}},"cell_type":"code","source":["# loss function\n","'''\n","Computes softmax cross entropy between logits and labels. (deprecated arguments)\n","'''\n","def loss_fn(model, images, labels):\n","  #model 인스턴스 생성\n","  logits = model(images, training=True)\n","  #cross entropy 손실함수를 사용해서 손실의 평균값을 구한다.\n","  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n","          logits=logits, labels=labels))\n","  return loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gIr9Cn4snvrY","colab_type":"code","colab":{}},"cell_type":"code","source":["# calculating gradient\n","def grad(model, images, labels):\n","  with tf.GradientTape() as tape:\n","    loss = loss_fn(model, images, labels)\n","  # model의 각 variable의 loss의 gradient값을 return한다.\n","  '''\n","  print('model variable type: {0}'.format(type(model.variables)))\n","  print(' ==== model variable list ==== ')\n","  [tf.keras.models.Sequential.variables]\n","  https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential#variables\n","  variables:\n","  Returns the list of all layer variables/weights.\n","\n","  Alias of self.weights.\n","\n","  Returns:\n","  A list of variables.\n","  '''\n","  return tape.gradient(loss, model.variables)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Wfw9qoB1oUuv","colab_type":"code","colab":{}},"cell_type":"code","source":["# calculating model's accuracy\n","def evaluate(model, images, labels):\n","  # model 인스턴스 생성\n","  logits = model(images, training=True)\n","  correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n","  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","  return accuracy"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zYvZU3cJr4N0","colab_type":"code","colab":{}},"cell_type":"code","source":["# Enable Eager Mode\n","tf.enable_eager_execution()\n","\n","# Hyper Parameters\n","learning_rate = 0.001\n","training_epochs = 15\n","batch_size = 100"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rUB_XMecaFaR","colab_type":"code","outputId":"8c41d958-0ef2-4725-8304-266264b8a1e4","executionInfo":{"status":"ok","timestamp":1555652366025,"user_tz":-540,"elapsed":1198,"user":{"displayName":"SoonYoung Jung","photoUrl":"","userId":"08526165032976493552"}},"colab":{"base_uri":"https://localhost:8080/","height":107}},"cell_type":"code","source":["## Dataset 처리\n","# MNIST Dataset\n","mnist = keras.datasets.mnist\n","class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n","\n","# load mnist dataset\n","(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n","\n","# image를 0~1 사이의 소수로 변환\n","train_images = train_images.astype(np.float32) / 255.\n","test_images = test_images.astype(np.float32) / 255.\n","print('shape train, test: {0}, {1}'.format(train_images.shape, test_images.shape))\n","\n","# 그레이스케일 이미지이므로 채널을 1로 차원을 확장 \n","train_images = np.expand_dims(train_images, axis=-1)\n","test_images = np.expand_dims(test_images, axis=-1)\n","print('shape train, test: {0}, {1}'.format(train_images.shape, test_images.shape))\n","\n","# to_categorical()함수는 숫자를 one-hotcoding 해준다.\n","print('train_labels[0]: {0}, type: {1}'.format(train_labels[0], type(train_labels[0])))\n","train_labels = to_categorical(train_labels, 10)\n","test_labels = to_categorical(test_labels, 10)\n","print('train_labels[0]: {0}, type: {1}'.format(train_labels[0], type(train_labels[0])))\n","\n","train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(\n","                buffer_size=100000).batch(batch_size)\n","test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(batch_size)\n","print('datashet shape: {0}'.format(type(train_dataset)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["shape train, test: (60000, 28, 28), (10000, 28, 28)\n","shape train, test: (60000, 28, 28, 1), (10000, 28, 28, 1)\n","train_labels[0]: 5, type: <class 'numpy.uint8'>\n","train_labels[0]: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], type: <class 'numpy.ndarray'>\n","datashet shape: <class 'tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter'>\n"],"name":"stdout"}]},{"metadata":{"id":"xk8gVaRDmp2X","colab_type":"code","outputId":"48c449d0-96dc-4498-bc50-c39ff76772ee","executionInfo":{"status":"ok","timestamp":1555652368479,"user_tz":-540,"elapsed":1023,"user":{"displayName":"SoonYoung Jung","photoUrl":"","userId":"08526165032976493552"}},"colab":{"base_uri":"https://localhost:8080/","height":503}},"cell_type":"code","source":["model = create_model()\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_24 (Conv2D)           (None, 28, 28, 32)        320       \n","_________________________________________________________________\n","max_pooling2d_24 (MaxPooling (None, 14, 14, 32)        0         \n","_________________________________________________________________\n","conv2d_25 (Conv2D)           (None, 14, 14, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_25 (MaxPooling (None, 7, 7, 64)          0         \n","_________________________________________________________________\n","conv2d_26 (Conv2D)           (None, 7, 7, 128)         73856     \n","_________________________________________________________________\n","max_pooling2d_26 (MaxPooling (None, 4, 4, 128)         0         \n","_________________________________________________________________\n","flatten_8 (Flatten)          (None, 2048)              0         \n","_________________________________________________________________\n","dense_16 (Dense)             (None, 256)               524544    \n","_________________________________________________________________\n","dropout_8 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_17 (Dense)             (None, 10)                2570      \n","=================================================================\n","Total params: 619,786\n","Trainable params: 619,786\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"3PCGD5HAoslO","colab_type":"code","colab":{}},"cell_type":"code","source":["# optimizer\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BD9S9li_pVnF","colab_type":"code","outputId":"c074385c-ee9f-40b3-8f2c-34583d4b844e","executionInfo":{"status":"ok","timestamp":1555652597652,"user_tz":-540,"elapsed":226406,"user":{"displayName":"SoonYoung Jung","photoUrl":"","userId":"08526165032976493552"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"cell_type":"code","source":["# train model\n","print('Learning started, It takes sometime.')\n","for epoch in range(training_epochs):\n","  avg_loss = 0.\n","  avg_train_acc = 0.\n","  avg_test_acc = 0.\n","  train_step = 0\n","  test_step = 0\n","  \n","  # train_dataset의 크기 만큼 반복, 순서대로 image, label쌍을 꺼낸다\n","  for images, labels in train_dataset:\n","    # training 용 image와 label의 손실의 gradient를 계산한다.\n","    grads = grad(model, images, labels)\n","    optimizer.apply_gradients(zip(grads, model.variables))\n","    loss = loss_fn(model, images, labels)\n","    # trainig accuracy를 측정하기 위해 매 횟수마다 총 합을 구한다.\n","    acc = evaluate(model, images, labels)\n","    avg_loss = avg_loss+loss\n","    avg_train_acc = avg_train_acc+acc\n","    train_step += 1\n","    \n","  # 손실, 정확도의 평균을 구한다.\n","  avg_loss = avg_loss / train_step\n","  avg_train_acc = avg_train_acc / train_step\n","  \n","  # test_dataset의 크기 만큼 반복, 순서대로 image, label쌍을 꺼낸다.\n","  for images, labels in test_dataset:\n","    acc = evaluate(model, images, labels)\n","    avg_test_acc = avg_test_acc + acc\n","    test_step += 1\n","  \n","  # test_dataset의 전체 정확도의 평균을 구한다.\n","  avg_test_acc = avg_test_acc / test_step\n","  \n","  print('Epoch:', '{}'.format(epoch + 1), 'loss =', '{:.8f}'.format(avg_loss), \n","          'train accuracy = ', '{:.4f}'.format(avg_train_acc), \n","          'test accuracy = ', '{:.4f}'.format(avg_test_acc))\n","print('Learning Finished!')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Learning started, It takes sometime.\n","Epoch: 1 loss = 0.16989940 train accuracy =  0.9476 test accuracy =  0.9829\n","Epoch: 2 loss = 0.04559535 train accuracy =  0.9860 test accuracy =  0.9824\n","Epoch: 3 loss = 0.03129787 train accuracy =  0.9898 test accuracy =  0.9872\n","Epoch: 4 loss = 0.02393986 train accuracy =  0.9926 test accuracy =  0.9884\n","Epoch: 5 loss = 0.01983924 train accuracy =  0.9941 test accuracy =  0.9904\n","Epoch: 6 loss = 0.01547315 train accuracy =  0.9951 test accuracy =  0.9889\n","Epoch: 7 loss = 0.01394321 train accuracy =  0.9955 test accuracy =  0.9877\n","Epoch: 8 loss = 0.01168457 train accuracy =  0.9966 test accuracy =  0.9911\n","Epoch: 9 loss = 0.00983523 train accuracy =  0.9969 test accuracy =  0.9899\n","Epoch: 10 loss = 0.00821953 train accuracy =  0.9975 test accuracy =  0.9894\n","Epoch: 11 loss = 0.00809129 train accuracy =  0.9973 test accuracy =  0.9918\n","Epoch: 12 loss = 0.00725020 train accuracy =  0.9976 test accuracy =  0.9898\n","Epoch: 13 loss = 0.00615880 train accuracy =  0.9978 test accuracy =  0.9907\n","Epoch: 14 loss = 0.00569178 train accuracy =  0.9982 test accuracy =  0.9901\n","Epoch: 15 loss = 0.00485916 train accuracy =  0.9981 test accuracy =  0.9921\n","Learning Finished!\n"],"name":"stdout"}]},{"metadata":{"id":"HHZnSWOA4OTk","colab_type":"text"},"cell_type":"markdown","source":[" - optimizer.apply_gradient()  \n"," optimizer.apply_gradient()와 optimizer.minimize()의 차이점에 대한 글  \n"," https://stackoverflow.com/questions/45473682/difference-between-apply-gradients-and-minimize-of-optimizer-in-tensorflow?rq=1\n"," \n","-  optimizer 구현에 대한 더 자세한 내용은 아래 링크를 참고해보자  \n"," https://www.tensorflow.org/api_docs/python/tf/train/Optimizer\n"," \n","- 다른 tutorial들이 여기에 있다. 참고해보자.  \n"," https://www.tensorflow.org/tutorials"]},{"metadata":{"id":"B2U55gmBsZ-y","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}